{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "df = pd.read_csv('/content/train.csv')\n",
        "\n",
        "positiveReviews = df[df['sentiment'] == 'positive']['review']\n",
        "negativeReviews = df[df['sentiment'] == 'negative']['review']\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_reviews(reviews):\n",
        "    lemmatized_words = []\n",
        "    for review in reviews:\n",
        "        review = review.lower()\n",
        "        review = re.sub(r'https?://\\S+|www\\.\\S+|@[A-Za-z0-9]+|#|\\d+', '', review)\n",
        "        tokens = word_tokenize(review)\n",
        "        lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "        lemmatized_words.extend(lemmatized)\n",
        "    return lemmatized_words\n",
        "\n",
        "lemmatized_positive = preprocess_reviews(positiveReviews)\n",
        "lemmatized_negative = preprocess_reviews(negativeReviews)\n",
        "\n",
        "positive_word_counts = Counter(lemmatized_positive)\n",
        "negative_word_counts = Counter(lemmatized_negative)\n",
        "\n",
        "top20_positive_words = positive_word_counts.most_common(20)\n",
        "top20_negative_words = negative_word_counts.most_common(20)\n",
        "\n",
        "def find_ngrams(input_list, n):\n",
        "    n_grams = ngrams(input_list, n)\n",
        "    return Counter(n_grams)\n",
        "\n",
        "positive_bigrams = find_ngrams(lemmatized_positive, 2)\n",
        "positive_trigrams = find_ngrams(lemmatized_positive, 3)\n",
        "\n",
        "print(\"Top 20 Positive Bigrams:\", positive_bigrams.most_common(20))\n",
        "print(\"Top 20 Positive Trigrams:\", positive_trigrams.most_common(20))\n",
        "\n",
        "negative_bigrams = find_ngrams(lemmatized_negative, 2)\n",
        "negative_trigrams = find_ngrams(lemmatized_negative, 3)\n",
        "\n",
        "print(\"Top 20 Negative Bigrams:\", negative_bigrams.most_common(20))\n",
        "print(\"Top 20 Negative Trigrams:\", negative_trigrams.most_common(20))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNaFv5iLsAnz",
        "outputId": "0d22c20e-be68-4666-b71e-404d54ff7626"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 20 Positive Bigrams: [(('/', '>'), 77752), (('<', 'br'), 77742), (('br', '/'), 77742), (('>', '<'), 38904), (('of', 'the'), 32863), ((',', 'and'), 25524), (('.', 'the'), 20995), (('in', 'the'), 20123), (('.', 'i'), 17576), ((',', 'but'), 16115), ((',', 'the'), 13638), (('it', \"'s\"), 13312), (('is', 'a'), 12790), (('.', 'it'), 12579), (('the', 'film'), 11688), (('and', 'the'), 11299), (('this', 'movie'), 10675), (('to', 'the'), 10031), (('it', 'is'), 9378), ((',', 'a'), 8715)]\n",
            "Top 20 Positive Trigrams: [(('<', 'br', '/'), 77742), (('br', '/', '>'), 77742), (('>', '<', 'br'), 38901), (('/', '>', '<'), 38899), (('/', '>', 'the'), 5831), (('.', '<', 'br'), 5033), (('one', 'of', 'the'), 4627), (('.', 'it', \"'s\"), 3805), (('/', '>', 'i'), 3163), (('.', 'this', 'is'), 2976), ((',', 'and', 'the'), 2881), (('!', '!', '!'), 2604), (('.', 'it', 'is'), 2517), (('this', 'is', 'a'), 2412), (('it', \"'s\", 'a'), 2226), (('of', 'the', 'film'), 2183), ((',', 'it', \"'s\"), 2047), (('a', 'lot', 'of'), 2013), (('/', '>', 'this'), 1958), ((',', 'but', 'it'), 1931)]\n",
            "Top 20 Negative Bigrams: [(('/', '>'), 82900), (('<', 'br'), 82893), (('br', '/'), 82893), (('>', '<'), 41457), (('of', 'the'), 28214), (('.', 'the'), 22340), ((',', 'and'), 21256), (('in', 'the'), 19591), (('.', 'i'), 18726), ((',', 'but'), 17242), (('this', 'movie'), 14122), (('it', \"'s\"), 13293), ((',', 'the'), 13155), (('.', 'it'), 11304), (('the', 'movie'), 10868), (('to', 'be'), 10663), (('the', 'film'), 9866), ((',', 'i'), 9738), (('and', 'the'), 9646), (('to', 'the'), 8657)]\n",
            "Top 20 Negative Trigrams: [(('<', 'br', '/'), 82893), (('br', '/', '>'), 82893), (('/', '>', '<'), 41457), (('>', '<', 'br'), 41457), (('/', '>', 'the'), 5948), (('.', '<', 'br'), 4817), (('/', '>', 'i'), 3824), (('.', 'it', \"'s\"), 3478), (('one', 'of', 'the'), 3163), (('i', 'do', \"n't\"), 2618), ((',', 'and', 'the'), 2602), (('!', '!', '!'), 2482), (('this', 'movie', 'is'), 2424), (('.', 'this', 'is'), 2236), ((',', 'it', \"'s\"), 2164), (('*', '*', '*'), 2139), (('of', 'the', 'movie'), 2038), (('of', 'the', 'film'), 1903), ((',', 'but', 'i'), 1782), ((',', 'but', 'it'), 1779)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def full_preprocess_reviews(reviews):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    preprocessed_reviews = []\n",
        "    for review in reviews:\n",
        "        review = re.sub(r'https?://\\S+|www\\.\\S+|@[A-Za-z0-9]+|#|\\d+', '', review)\n",
        "        review = re.sub(r'[^\\w\\s]', '', review)\n",
        "        review = review.lower()\n",
        "        tokens = word_tokenize(review)\n",
        "        lemmatized = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "        preprocessed_reviews.append(' '.join(lemmatized))\n",
        "    return preprocessed_reviews\n",
        "\n",
        "preprocessed_positive = full_preprocess_reviews(positiveReviews)\n",
        "preprocessed_negative = full_preprocess_reviews(negativeReviews)\n"
      ],
      "metadata": {
        "id": "f1HTuHgbu7Yb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
        "import pandas as pd\n",
        "\n",
        "df_preprocessed = pd.DataFrame({\n",
        "    'text': preprocessed_positive + preprocessed_negative,\n",
        "    'target': [1] * len(preprocessed_positive) + [0] * len(preprocessed_negative)\n",
        "})\n",
        "\n",
        "X = df_preprocessed['text']\n",
        "y = df_preprocessed['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "def train_evaluate_logistic_regression(max_features):\n",
        "    vectorizer = CountVectorizer(max_features=max_features)\n",
        "    X_train_vect = vectorizer.fit_transform(X_train)\n",
        "    X_test_vect = vectorizer.transform(X_test)\n",
        "\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train_vect, y_train)\n",
        "    predictions = model.predict(X_test_vect)\n",
        "\n",
        "    print(f\"Max Features: {max_features}\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
        "    print(\"F1 Score:\", f1_score(y_test, predictions))\n",
        "    print(\"Recall:\", recall_score(y_test, predictions))\n",
        "    print(\"\")\n",
        "\n",
        "train_evaluate_logistic_regression(100)\n",
        "train_evaluate_logistic_regression(1000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyxR3-xNu877",
        "outputId": "6c3de515-24a7-41cb-8c9d-1789d47b027f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Features: 100\n",
            "Accuracy: 0.738125\n",
            "F1 Score: 0.7424708051628766\n",
            "Recall: 0.7538691962056915\n",
            "\n",
            "Max Features: 1000\n",
            "Accuracy: 0.852125\n",
            "F1 Score: 0.8529155787641426\n",
            "Recall: 0.8562156764852721\n",
            "\n"
          ]
        }
      ]
    }
  ]
}